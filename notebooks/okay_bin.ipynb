{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 28,
            "source": [
                "import pathlib\n",
                "from Bio import SeqIO\n",
                "import pandas as pd\n",
                "import pickle\n",
                "import numpy as np\n",
                "import multiprocessing as mp"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Try to create binning code interactively - \n",
                "## i1 is the intermediate results right after classify()   i2 is the results at the end."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Notes for this part: \n",
                "1. If I'm gonna use a dataframe and all that might as well make the columns and format it prior to binning.\n",
                "2. If not make sure to think of alternative."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "# Define output folder from a Strainr run\n",
                "\n",
                "output_folder = pathlib.Path('howdy4/')\n",
                "\n",
                "# Extract intermediate from pkl\n",
                "i1 = output_folder / 'i1.pkl'\n",
                "with i1.open('rb') as ph:\n",
                "    dfp1 = pickle.load(ph)\n",
                "\n",
                "# Extract final from pickle\n",
                "i2 = output_folder / 'i2.pkl'\n",
                "with i2.open('rb') as ph:\n",
                "    dfp2 = pickle.load(ph)\n",
                "\n",
                "# Get the strain list for columns\n",
                "with (output_folder / 'strain_list.txt').open('r') as sh:\n",
                "    strain_list = []\n",
                "    for line in sh:\n",
                "        strain_list.append(line.strip())\n",
                "        \n",
                "# Convert to dataframe (for now)\n",
                "df1 = pd.DataFrame.from_dict(dict(dfp1)).T\n",
                "df2 = pd.Series(dict(dfp2))\n",
                "df1.columns = strain_list"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "count    90667.000000\n",
                            "mean        39.616729\n",
                            "std         17.794168\n",
                            "min          1.000000\n",
                            "25%         27.000000\n",
                            "50%         27.000000\n",
                            "75%         61.000000\n",
                            "max         72.000000\n",
                            "dtype: float64"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 3
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "source": [
                "f1 = pathlib.Path('/home/mladen/Strainr/test_R1.fastq')\n",
                "f2 = pathlib.Path('/home/mladen/Strainr/test_R2.fastq')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Use final results to get the top strain candidates"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "source": [
                "top_strain_indices = list(df2.value_counts().index[:5])\n",
                "top_strain_names = [strain_list[i] for i in top_strain_indices]\n",
                "top_strain_names"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['GCA_000027305.1 Haemophilus influenzae Rd KW20',\n",
                            " 'GCA_000698365.1 Haemophilus influenzae CGSHiCZ412602',\n",
                            " 'GCA_000968335.1 Haemophilus influenzae 2019',\n",
                            " 'GCA_000197875.1 Haemophilus influenzae F3031',\n",
                            " 'GCA_000016465.1 Haemophilus influenzae PittEE']"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 26
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# New code for multiple bins"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "source": [
                "def bin_helper(top_strain_names, df1, f1, f2, outdir):\n",
                "    \"\"\" strain_dict is the strain_id : set of reads\"\"\"\n",
                "\n",
                "    # Make a directory for output\n",
                "    bin_dir = outdir / 'bins'\n",
                "    bin_dir.mkdir(exist_ok=True,parents=True)\n",
                "\n",
                "    nbins = 2\n",
                "    print(f\"Generating sequence files for the top {nbins} strains.\")\n",
                "    # most_reads_frame = (df1 != 0).sum().sort_values(ascending=False)\n",
                "\n",
                "    procs, saved_strains = [], []\n",
                "    for strain_id in top_strain_names:\n",
                "        if strain_id == \"NA\":\n",
                "            continue\n",
                "\n",
                "        # Keep track of binned strains\n",
                "        saved_strains.append(strain_id)\n",
                "        print(f\"Binning reads for {strain_id}...\")\n",
                "        p = mp.Process( target=bin_single, args=( strain_id, df1, f1, f2, output_folder, ),)\n",
                "        p.start()\n",
                "        procs.append(p)\n",
                "\n",
                "    [p.join() for p in procs]\n",
                "\n",
                "    # Return the set of strains and the list of processes\n",
                "    return set(saved_strains), procs\n",
                "\n",
                "bin_helper( top_strain_names,df1, f1, f2, output_folder)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Generating sequence files for the top 2 strains.\n",
                        "Binning reads for GCA_000027305.1 Haemophilus influenzae Rd KW20...\n",
                        "Binning reads for GCA_000698365.1 Haemophilus influenzae CGSHiCZ412602...\n",
                        "Binning reads for GCA_000968335.1 Haemophilus influenzae 2019...\n",
                        "Binning reads for GCA_000197875.1 Haemophilus influenzae F3031...\n",
                        "Binning reads for GCA_000016465.1 Haemophilus influenzae PittEE...\n",
                        "Saved 209122 records from /home/mladen/Strainr/test_R1.fastq to howdy4/bin.GCA_000968335.1_R1.fastq\n",
                        "Saved 268345 records from /home/mladen/Strainr/test_R1.fastq to howdy4/bin.GCA_000027305.1_R1.fastq\n",
                        "Saved 267901 records from /home/mladen/Strainr/test_R1.fastq to howdy4/bin.GCA_000016465.1_R1.fastq\n",
                        "Saved 271668 records from /home/mladen/Strainr/test_R1.fastq to howdy4/bin.GCA_000698365.1_R1.fastq\n",
                        "Saved 317174 records from /home/mladen/Strainr/test_R1.fastq to howdy4/bin.GCA_000197875.1_R1.fastq\n",
                        "Saved 209122 records from /home/mladen/Strainr/test_R2.fastq to howdy4/bin.GCA_000968335.1_R2.fastq\n",
                        "Saved 268345 records from /home/mladen/Strainr/test_R2.fastq to howdy4/bin.GCA_000027305.1_R2.fastq\n",
                        "Saved 267901 records from /home/mladen/Strainr/test_R2.fastq to howdy4/bin.GCA_000016465.1_R2.fastq\n",
                        "Saved 271668 records from /home/mladen/Strainr/test_R2.fastq to howdy4/bin.GCA_000698365.1_R2.fastq\n",
                        "Saved 317174 records from /home/mladen/Strainr/test_R2.fastq to howdy4/bin.GCA_000197875.1_R2.fastq\n"
                    ]
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "({'GCA_000016465.1 Haemophilus influenzae PittEE',\n",
                            "  'GCA_000027305.1 Haemophilus influenzae Rd KW20',\n",
                            "  'GCA_000197875.1 Haemophilus influenzae F3031',\n",
                            "  'GCA_000698365.1 Haemophilus influenzae CGSHiCZ412602',\n",
                            "  'GCA_000968335.1 Haemophilus influenzae 2019'},\n",
                            " [<Process name='Process-6' pid=449648 parent=448350 stopped exitcode=0>,\n",
                            "  <Process name='Process-7' pid=449649 parent=448350 stopped exitcode=0>,\n",
                            "  <Process name='Process-8' pid=449650 parent=448350 stopped exitcode=0>,\n",
                            "  <Process name='Process-9' pid=449651 parent=448350 stopped exitcode=0>,\n",
                            "  <Process name='Process-10' pid=449652 parent=448350 stopped exitcode=0>])"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 44
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Single bin"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "source": [
                "\n",
                "def bin_single( strain , df, forward_file, reverse_file, bin_dir):\n",
                "    \"\"\" Given a strain, use hit info to extract reads\"\"\"\n",
                "    strain_accession = strain.split()[0]\n",
                "    paired_files = [forward_file,reverse_file]\n",
                "\n",
                "    for fidx, input_file in enumerate(paired_files): # (0,R1), (1,R2)\n",
                "        writefile_name = f\"bin.{strain_accession}_R{fidx+1}.fastq\"\n",
                "        writefile = bin_dir / writefile_name\n",
                "\n",
                "        if fidx == 0:\n",
                "            reads = set(df[df[strain] > 0].index)\n",
                "        else:\n",
                "            reads = set(df[df[strain] > 0].index.str.replace('/1','/2'))\n",
                "            \n",
                "        with input_file.open('r') as original, writefile.open('w') as newfasta:\n",
                "            records = (r for r in SeqIO.parse(original,'fastq') if r.id in reads)\n",
                "            count = SeqIO.write(records, newfasta, \"fastq\")\n",
                "\n",
                "        print(f\"Saved {count} records from {input_file} to {writefile}\")\n",
                "\n",
                "    return\n",
                "bin_single(strain_to_bin , df1, f1, f2, output_folder)"
            ],
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "NameError",
                    "evalue": "name 'strain_to_bin' is not defined",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipykernel_448350/3428906601.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mbin_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrain_to_bin\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m: name 'strain_to_bin' is not defined"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Selecting strains the previous way - getting number of reads associated with each strain"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "source": [
                "import numpy as np\n",
                "(df1 != 0).sum().sort_values(ascending=False)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Annotate the old code"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def write_strain_fastas( strain_dict, f1, f2,):\n",
                "\n",
                "    \"\"\" strain_dict is the strain_id : set of reads\"\"\"\n",
                "\n",
                "    # Make a directory for output\n",
                "    fastadir = outdir / 'fastas'\n",
                "    fastadir.mkdir(exist_ok=True,parents=True)\n",
                "\n",
                "    # Get the number of bins to perform\n",
                "    max_n = len(topstrains)\n",
                "    # print(f\"Generating sequence files for the top {max_n} strains.\")\n",
                "\n",
                "    # Sort and select the top bins from the sorted list\n",
                "    # strain_dict = dict(sorted(strain_dict.items(), key=lambda item: item[1])[:max_n])\n",
                "\n",
                "    # Loop through each strain to make the file\n",
                "    procs, saved_strains = [], []\n",
                "    for strain_id, read_set in strain_dict.items():\n",
                "\n",
                "        # Don't bin NA\n",
                "        if strain_id == \"NA\":\n",
                "            continue\n",
                "        \n",
                "        # Keep track of binned strains\n",
                "        saved_strains.append(strain_id)\n",
                "        print(f\"Writing {len(read_set)} reads for {strain_id}...\")\n",
                "\n",
                "        # Call the process with dict key + val, file1 + file2, output\n",
                "        p = Process( target=mp_write_single_strain, args=( strain_id, read_set, f1, f2, fastadir, ),)\n",
                "\n",
                "        # Begin the process\n",
                "        p.start()\n",
                "        \n",
                "        # Make a process list\n",
                "        procs.append(p)\n",
                "\n",
                "    # Collect processes\n",
                "    # [p.join() for p in procs]\n",
                "\n",
                "    # Return the set of strains and the list of processes\n",
                "    return set(saved_strains), procs\n",
                "    \n",
                "    \n",
                "def mp_write_single_strain( strain , read_set, forward_file, reverse_file,  fastadir , ):\n",
                "\n",
                "    paired_files = [forward_file,reverse_file]\n",
                "    fext = \"fastq\"\n",
                "\n",
                "    # Loop through forward and reverse forward=0, reverse=1\n",
                "    for fidx, input_file in enumerate(paired_files): # (0,R1), (1,R2)\n",
                "\n",
                "        # Set up list of reads + string-replace\n",
                "        if fidx == 1: #reverse\n",
                "            read_names = set( read.replace(\"/1\", \"/2\") for read in read_set)\n",
                "        else:\n",
                "            read_names = set(read for read in read_set)\n",
                "\n",
                "\n",
                "        # Set up output file\n",
                "        writefile_name = f\"sbin.{strain}_R{fidx+1}.{fext}\"  \n",
                "        writefile = fastadir / writefile_name\n",
                "\n",
                "        # Open input, find reads that match dict, write to strain_fasta\n",
                "\n",
                "        with _open(input_file) as rhandle, open(writefile, \"w\") as whandle:\n",
                "            # Open original fasta, open future fasta\n",
                "\n",
                "            # Collect SeqIO records (id,dna,qual) that are in the read_set\n",
                "            records = (r for r in SeqIO.parse(rhandle,'fastq') if r in read_names)\n",
                "            # write them to a file\n",
                "            count = SeqIO.write(records, whandle, \"fastq\")\n",
                "            print(f\"Saved {count} records from {input_file} to {writefile}\")\n",
                "\n",
                "    return\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Old Code"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\n",
                "class Binner:\n",
                "    def __init__(self, count_table, min_hit_threshold, min_k_threshold, final_hits,f1, f2):\n",
                "        self.table = count_table\n",
                "        self.min_hit = min_hit_threshold\n",
                "        self.min_k = min_k_threshold\n",
                "        self.final_hits = final_hits\n",
                "        self.f1 = f1\n",
                "        self.f2 = f2\n",
                "        \n",
                "        \n",
                "        \n",
                "        \n",
                "\n",
                "def filter_strains_bin(strain_map:dict, minhits: int,fhits: dict):\n",
                "    return { k: v for k, v in strain_map.items() \n",
                "            if len(fhits[k]) > minhits \n",
                "            }\n",
                "\n",
                "def bin_stuff(intermediate, fhits):\n",
                "    minhits = 1000\n",
                "    df = pd.read_results('intermed.csv').set_index('Unnamed: 0')\n",
                "\n",
                "    strain_map = strain_to_reads_map(intermediate, min_k=1)\n",
                "    print(strain_map)\n",
                "    strain_map = filter_strains_bin(strain_map, minhits, fhits)\n",
                "    print(strain_map)\n",
                "    topstrains = select_top_bins(df,nstrains,min_k)\n",
                "    print(topstrains)\n",
                "    f2 = params['reverse']\n",
                "    print(f2)\n",
                "    astrain_set, aplist = write_strain_fastas( strain_map, f1, f2)\n",
                "    print(astrain_set)\n",
                "    print(aplist)\n",
                "    [ap.join() for ap in aplist]\n",
                "    return\n",
                "\n",
                "def select_top_bins(df,nstrains,min_k):\n",
                "    min_k= 90\n",
                "    nstrains = 5\n",
                "    strain_nhits = {}\n",
                "    for strain in df.columns:\n",
                "        nreads = len(list(df[df[strain] > min_k].index))\n",
                "        strain_nhits[strain] = nreads\n",
                "    #     print(f\"The strain {strain} has {nreads} reads with greater than {min_k} k-mer hits\") \n",
                "    topstrains = sorted(strain_nhits.items(),key=lambda kv: kv[1],reverse=True)[:nstrains]\n",
                "    return [i[0] for i in topstrains]\n",
                "\n",
                "def write_strain_fastas( strain_dict, f1, f2,):\n",
                "    fastadir = outdir / 'fastas'\n",
                "    fastadir.mkdir(exist_ok=True,parents=True)\n",
                "    max_n = len(topstrains)\n",
                "\n",
                "    # print(f\"Generating sequence files for the top {max_n} strains.\")\n",
                "    # strain_dict = dict(sorted(strain_dict.items(), key=lambda item: item[1])[:max_n])\n",
                "\n",
                "    # Loop through each strain to make the file\n",
                "    procs, saved_strains = [], []\n",
                "    for strain_id, read_set in strain_dict.items():\n",
                "        if strain_id == \"NA\":\n",
                "            continue\n",
                "\n",
                "        saved_strains.append(strain_id)\n",
                "        print(f\"Writing {len(read_set)} reads for {strain_id}...\")\n",
                "        p = Process( target=mp_write_single_strain, args=( strain_id, read_set, f1, f2, fastadir, ),)\n",
                "        p.start()\n",
                "        procs.append(p)\n",
                "\n",
                "    # [p.join() for p in procs]\n",
                "    return set(saved_strains), procs\n",
                "\n",
                "    \n",
                "    \n",
                "def mp_write_single_strain( strain , read_set, forward_file, reverse_file,  fastadir , ):\n",
                "\n",
                "    paired_files = [forward_file,reverse_file]\n",
                "    fext = \"fastq\"\n",
                "    for fidx, input_file in enumerate(paired_files): # (0,R1), (1,R2)\n",
                "        if fidx == 1: #reverse\n",
                "            read_names = set( read.replace(\"/1\", \"/2\") for read in read_set)\n",
                "        else:\n",
                "            read_names = set(read for read in read_set)\n",
                "\n",
                "        writefile_name = f\"sbin.{strain}_R{fidx+1}.{fext}\"  \n",
                "        writefile = fastadir / writefile_name\n",
                "\n",
                "        # Open input, find reads that match dict, write to strain_fasta\n",
                "        with _open(input_file) as rhandle, open(writefile, \"w\") as whandle:\n",
                "            records = (r for r in SeqIO.parse(rhandle,'fastq') if r in read_names)\n",
                "            count = SeqIO.write(records, whandle, \"fastq\")\n",
                "            print(f\"Saved {count} records from {input_file} to {writefile}\")\n",
                "\n",
                "    return\n",
                "\n",
                "\n",
                "\"\"\"\n",
                "import pandas as pd\n",
                "df = pd.read_csv('intermed.csv').set_index('Unnamed: 0')\n",
                "\n",
                "        \n",
                "def select_top_bins(df,nstrains,min_k):\n",
                "    min_k= 90\n",
                "    nstrains = 5\n",
                "    strain_nhits = {}\n",
                "    for strain in df.columns:\n",
                "        nreads = len(list(df[df[strain] > min_k].index))\n",
                "        strain_nhits[strain] = nreads\n",
                "    #     print(f\"The strain {strain} has {nreads} reads with greater than {min_k} k-mer hits\") \n",
                "    topstrains = sorted(strain_nhits.items(),key=lambda kv: kv[1],reverse=True)\n",
                "    topstrains = topstrains[:nstrains]\n",
                "    return [i[0] for i in topstrains]\n",
                "\n",
                "        \n",
                "\"\"\""
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.6 64-bit ('datasci': conda)"
        },
        "interpreter": {
            "hash": "054e97ab1499b4094c6cfd25e169e17b295600d4a890dffc31f0e6961bd5548e"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}